{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\n",
      "`load_boston` has been removed from scikit-learn since version 1.2.\n",
      "\n",
      "The Boston housing prices dataset has an ethical problem: as\n",
      "investigated in [1], the authors of this dataset engineered a\n",
      "non-invertible variable \"B\" assuming that racial self-segregation had a\n",
      "positive impact on house prices [2]. Furthermore the goal of the\n",
      "research that led to the creation of this dataset was to study the\n",
      "impact of air quality but it did not give adequate demonstration of the\n",
      "validity of this assumption.\n",
      "\n",
      "The scikit-learn maintainers therefore strongly discourage the use of\n",
      "this dataset unless the purpose of the code is to study and educate\n",
      "about ethical issues in data science and machine learning.\n",
      "\n",
      "In this special case, you can fetch the dataset from the original\n",
      "source::\n",
      "\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "    target = raw_df.values[1::2, 2]\n",
      "\n",
      "Alternative datasets include the California housing dataset and the\n",
      "Ames housing dataset. You can load the datasets as follows::\n",
      "\n",
      "    from sklearn.datasets import fetch_california_housing\n",
      "    housing = fetch_california_housing()\n",
      "\n",
      "for the California housing dataset and::\n",
      "\n",
      "    from sklearn.datasets import fetch_openml\n",
      "    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "for the Ames housing dataset.\n",
      "\n",
      "[1] M Carlisle.\n",
      "\"Racist data destruction?\"\n",
      "<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n",
      "\n",
      "[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n",
      "\"Hedonic housing prices and the demand for clean air.\"\n",
      "Journal of environmental economics and management 5.1 (1978): 81-102.\n",
      "<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
      ": LawSchoolGPADataset will be unavailable. To install, run:\n",
      "pip install 'aif360[LawSchoolGPA]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "class DisparateImpactRemovalClassifier(BaseEstimator):\n",
    "\n",
    "    def __init__(self, estimator, repair_level:int=1.0, verbose=False, random_state=None):\n",
    "        assert isinstance(estimator, ClassifierMixin), \"estimator must be a classifier\"\n",
    "        assert repair_level >= 0.0 and repair_level <= 1.0, \"repair_level must be in [0, 1]\"\n",
    "        assert isinstance(verbose, bool), \"verbose must be a boolean\"\n",
    "        assert isinstance(random_state, int) or random_state is None, \"random_state must be an integer or None\"\n",
    "                \n",
    "        self.estimator = estimator\n",
    "        self.repair_level = repair_level\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self.set_params(random_state=random_state)\n",
    "\n",
    "    def set_params(self, **kwargs):\n",
    "        try:\n",
    "            self.estimator.set_params(**kwargs)\n",
    "        except:\n",
    "            pass\n",
    " \n",
    "    def fit(self, X, y, sensitive_features):\n",
    "        self.preprocessor = DisparateImpactRemover(\n",
    "            repair_level=self.repair_level, sensitive_attribute=0\n",
    "        )\n",
    "        X_processed = self.remove_bias(X, sensitive_features)\n",
    "        self.estimator.fit(X_processed, y)\n",
    "        return self\n",
    "\n",
    "    def remove_bias(self, X, sensitive_features):\n",
    "        \"\"\"Remove bias from X using the DisparateImpactRemover preprocessor.\"\"\"\n",
    "        assert (X[:, 0] == sensitive_features).all(), \\\n",
    "            \"The 1st column of X must be the sensitive attribute.\"\n",
    "        \n",
    "        y_dummy = np.zeros(X.shape[0])\n",
    "        aif360_data = BinaryLabelDataset(\n",
    "            df=pd.DataFrame(np.hstack([X, y_dummy.reshape(-1, 1)])), \n",
    "            label_names=[X.shape[1]],\n",
    "            protected_attribute_names=[0],\n",
    "            privileged_protected_attributes = [[1.0]],\n",
    "            unprivileged_protected_attributes = [[0.0]],\n",
    "        )\n",
    "        X_processed = self.preprocessor.fit_transform(aif360_data).features\n",
    "        return X_processed\n",
    "\n",
    "    def predict(self, X, sensitive_features):\n",
    "        X_processed = self.remove_bias(X, sensitive_features)\n",
    "        return self.estimator.predict(X_processed)\n",
    "\n",
    "    def predict_proba(self, X, sensitive_features):\n",
    "        X_processed = self.remove_bias(X, sensitive_features)\n",
    "        return self.estimator.predict_proba(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from aif360.algorithms.preprocessing import LFR\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "class LFRClassifer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, estimator, privileged_groups, unprivileged_groups, \n",
    "        k=10, Ax=0.1, Ay=1.0, Az=2.0, maxiter=1000, maxfun=1000,\n",
    "        print_interval=100, verbose=False, random_state=None,):\n",
    "        assert isinstance(estimator, ClassifierMixin), \"estimator must be a classifier\"\n",
    "        assert isinstance(verbose, bool), \"verbose must be a boolean\"\n",
    "        assert isinstance(random_state, int) or random_state is None, \"random_state must be an integer or None\"\n",
    "                \n",
    "        self.estimator = estimator\n",
    "        self.privileged_groups = privileged_groups\n",
    "        self.unprivileged_groups = unprivileged_groups\n",
    "        self.k = k\n",
    "        self.Ax = Ax\n",
    "        self.Ay = Ay\n",
    "        self.Az = Az\n",
    "        self.print_interval = print_interval\n",
    "        self.maxiter = maxiter\n",
    "        self.maxfun = maxfun            \n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self.set_params(random_state=random_state)\n",
    "        self.preprocessor = LFR(\n",
    "            privileged_groups=privileged_groups,\n",
    "            unprivileged_groups=unprivileged_groups,\n",
    "            k=k,\n",
    "            Ax=Ax,\n",
    "            Ay=Ay,\n",
    "            Az=Az,\n",
    "            print_interval=100,\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "        self.preprocessor_fit_kwargs = {\n",
    "            'maxiter': maxiter,\n",
    "            'maxfun': maxfun,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **kwargs):\n",
    "        try:\n",
    "            self.estimator.set_params(**kwargs)\n",
    "        except:\n",
    "            pass\n",
    " \n",
    "    def fit(self, X, y, sensitive_features):\n",
    "        self.fit_preprocessor(X, y, sensitive_features)\n",
    "        X_, y_ = self.transform_data(X, sensitive_features, y=y, return_y=True)\n",
    "        try:\n",
    "            self.estimator.fit(X_, y_)\n",
    "        except:\n",
    "            self.estimator.fit(X_, y)\n",
    "        return self\n",
    "\n",
    "    def translate_into_aif360_dataset(self, X, sensitive_features, y=None):\n",
    "        if y is None:\n",
    "            y = np.ones(X.shape[0])\n",
    "        aif360_data = BinaryLabelDataset(\n",
    "            df=pd.DataFrame(np.hstack([X, y.reshape(-1, 1)])), \n",
    "            label_names=[X.shape[1]],\n",
    "            protected_attribute_names=[0],\n",
    "            privileged_protected_attributes = [[1.0]],\n",
    "            unprivileged_protected_attributes = [[0.0]],\n",
    "        )\n",
    "        return aif360_data\n",
    "\n",
    "    def fit_preprocessor(self, X, y, sensitive_features):\n",
    "        \"\"\"Remove bias from X using the DisparateImpactRemover preprocessor.\"\"\"\n",
    "        assert (X[:, 0] == sensitive_features).all(), \\\n",
    "            \"The 1st column of X must be the sensitive attribute.\"\n",
    "        assert y is not None, \"y must be provided to fit the LFR preprocessor.\"\n",
    "\n",
    "        aif360_data = self.translate_into_aif360_dataset(X, sensitive_features, y=y)\n",
    "        self.preprocessor.fit(aif360_data, **self.preprocessor_fit_kwargs)\n",
    "        return\n",
    "    \n",
    "    def transform_data(self, X, sensitive_features, y=None, return_y=False):\n",
    "        aif360_data = self.translate_into_aif360_dataset(X, sensitive_features, y=y)\n",
    "        X_transformed = self.preprocessor.transform(aif360_data).features\n",
    "        y_transformed = self.preprocessor.transform(aif360_data).labels.ravel()\n",
    "        if return_y:\n",
    "            return X_transformed, y_transformed\n",
    "        else:\n",
    "            return X_transformed\n",
    "\n",
    "    def predict(self, X, sensitive_features):\n",
    "        X_processed = self.transform_data(X, sensitive_features)\n",
    "        return self.estimator.predict(X_processed)\n",
    "\n",
    "    def predict_proba(self, X, sensitive_features):\n",
    "        X_processed = self.transform_data(X, sensitive_features)\n",
    "        return self.estimator.predict_proba(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import shap\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import base classifiers\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from baselines import AdaFairClassifier\n",
    "from imbens.ensemble import SMOTEBoostClassifier, SMOTEBaggingClassifier, RUSBoostClassifier, UnderBaggingClassifier, SelfPacedEnsembleClassifier\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairens import FairAugEnsemble, FairEnsemble\n",
    "\n",
    "# Import utilities\n",
    "from data import FairDataset    # This is a custom class that we will use to load the datasets\n",
    "from eval import evaluate_multi_split, verbose_print\n",
    "from trainer import Trainer\n",
    "from utils import seed_generator, dict_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset    : compas (5875, 12) load from ./data/compas.csv\n",
      "Sens/Res   : sex/label\n",
      "Split      : train/test = 0.8/0.2, random_state = 42, x_with_s = True\n",
      "train      | size {0: 929, 1: 3771} | grp_pos_ratio: {0: 0.3617, 1: 0.4916}\n",
      "test       | size {0: 232, 1: 943} | grp_pos_ratio: {0: 0.3621, 1: 0.491}\n",
      "\n",
      "Dataset    : compas (5875, 12) load from ./data/compas.csv\n",
      "Sens/Res   : race/label\n",
      "Split      : train/test = 0.8/0.2, random_state = 42, x_with_s = True\n",
      "train      | size {0: 1878, 1: 2822} | grp_pos_ratio: {0: 0.3946, 1: 0.5135}\n",
      "test       | size {0: 469, 1: 706} | grp_pos_ratio: {0: 0.3945, 1: 0.5127}\n",
      "\n",
      "Dataset    : adult (45222, 99) load from ./data/adult.csv\n",
      "Sens/Res   : gender/label\n",
      "Split      : train/test = 0.8/0.2, random_state = 42, x_with_s = True\n",
      "train      | size {0: 11756, 1: 24421} | grp_pos_ratio: {0: 0.1136, 1: 0.3125}\n",
      "test       | size {0: 2939, 1: 6106} | grp_pos_ratio: {0: 0.1136, 1: 0.3125}\n",
      "\n",
      "Dataset    : adult (45222, 99) load from ./data/adult.csv\n",
      "Sens/Res   : race/label\n",
      "Split      : train/test = 0.8/0.2, random_state = 42, x_with_s = True\n",
      "train      | size {0: 5055, 1: 31122} | grp_pos_ratio: {0: 0.1585, 1: 0.2624}\n",
      "test       | size {0: 1264, 1: 7781} | grp_pos_ratio: {0: 0.1582, 1: 0.2623}\n",
      "\n",
      "Dataset    : bank (30488, 58) load from ./data/bank.csv\n",
      "Sens/Res   : age/label\n",
      "Split      : train/test = 0.8/0.2, random_state = 42, x_with_s = True\n",
      "train      | size {0: 23699, 1: 691} | grp_pos_ratio: {0: 0.1235, 1: 0.2301}\n",
      "test       | size {0: 5925, 1: 173} | grp_pos_ratio: {0: 0.1235, 1: 0.2312}\n",
      "\n",
      "Dataset    : bank (30488, 58) load from ./data/bank.csv\n",
      "Sens/Res   : marital=married/label\n",
      "Split      : train/test = 0.8/0.2, random_state = 42, x_with_s = True\n",
      "train      | size {0: 13994, 1: 10396} | grp_pos_ratio: {0: 0.1176, 1: 0.1387}\n",
      "test       | size {0: 3498, 1: 2600} | grp_pos_ratio: {0: 0.1175, 1: 0.1388}\n",
      "\n",
      "////// Dataset ZOO //////\n",
      "compas_sex: <data.FairDataset object at 0x000001EA721F7640>\n",
      "compas_race: <data.FairDataset object at 0x000001EA6CF5F640>\n",
      "adult_gender: <data.FairDataset object at 0x000001EA6CF5F7C0>\n",
      "adult_race: <data.FairDataset object at 0x000001EA6CF5F670>\n",
      "bank_age: <data.FairDataset object at 0x000001EA08503730>\n",
      "bank_marital=married: <data.FairDataset object at 0x000001EA0863FB50>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load Datasets\"\"\"\n",
    "\n",
    "dataset_kwargs = {\n",
    "    'y_col': 'label',\n",
    "    'train_size': 0.6,\n",
    "    'val_size': 0.2,\n",
    "    'test_size': 0.2,\n",
    "    'concat_train_val': True,\n",
    "    'normalize': True,\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "all_datasets = {\n",
    "    'compas': ['sex', 'race'],\n",
    "    'adult': ['gender', 'race'],\n",
    "    'bank': ['age', 'marital=married'],\n",
    "    # 'lsa_unfair_gender_race': ['gender', 'race'],\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Create a dictionary of datasets: dataset_zoo\n",
    "key: dataset name\n",
    "value: FairDataset object\n",
    "\"\"\"\n",
    "dataset_zoo = {}\n",
    "for dataname, s_attrs in all_datasets.items():\n",
    "    for s_attr in s_attrs:\n",
    "        dataset = FairDataset(\n",
    "            dataname=dataname,\n",
    "            csv_path=f'./data/{dataname}.csv',\n",
    "            s_col=s_attr,\n",
    "            **dataset_kwargs\n",
    "        )\n",
    "        dataset_zoo[dataset.fullname] = dataset\n",
    "\n",
    "        # dataset.describe()\n",
    "        dataset.brief()\n",
    "\n",
    "# Print the information of the datasets and models\n",
    "print(\n",
    "    f\"////// Dataset ZOO //////\\n\"\n",
    "    f\"{dict_info(dataset_zoo)}\\n\"\n",
    ")\n",
    "\n",
    "dataset_zoo_subset = {\n",
    "    'compas_sex': dataset_zoo['compas_sex'],\n",
    "    'compas_race': dataset_zoo['compas_race'],\n",
    "    'adult_gender': dataset_zoo['adult_gender'],\n",
    "    'adult_race': dataset_zoo['adult_race'],\n",
    "    'bank_age': dataset_zoo['bank_age'],\n",
    "    'bank_marital=married': dataset_zoo['bank_marital=married'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Benchmarker with:\n",
      "Random seed: 42\n",
      "Base models: ['LR', 'DT', 'MLP']\n",
      "Techniques:  ['UnderBag']\n",
      "Datasets:    ['compas_sex', 'compas_race', 'adult_gender', 'adult_race', 'bank_age', 'bank_marital=married']\n",
      "# models:    6\n",
      "# datasets:  6\n",
      "\n",
      "Running All models ...\n",
      "========== Start Running on Dataset: compas_sex ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data: compas_sex | Model: Dummy        :   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data: compas_sex | Model: Dummy        : 100%|██████████| 5/5 [00:00<00:00,  9.07it/s, ACC 0.497±0.021 | BACC 0.495±0.022 | DP 0.010±0.006 | EO 0.044±0.043 | SI 0.000±0.000 | AdvG {0: 4, 1: 1}]\n",
      "Data: compas_sex | Model: LR           : 100%|██████████| 5/5 [00:00<00:00,  8.56it/s, ACC 0.696±0.006 | BACC 0.690±0.006 | DP 0.336±0.016 | EO 0.382±0.039 | SI 0.210±0.013 | AdvG {1: 5}]\n",
      "Data: compas_sex | Model: LR_UnderBag  : 100%|██████████| 5/5 [00:01<00:00,  4.27it/s, ACC 0.688±0.011 | BACC 0.688±0.011 | DP 0.377±0.034 | EO 0.371±0.057 | SI 0.248±0.013 | AdvG {1: 5}]\n",
      "Data: compas_sex | Model: DT           : 100%|██████████| 5/5 [00:00<00:00,  8.04it/s, ACC 0.607±0.013 | BACC 0.603±0.013 | DP 0.085±0.037 | EO 0.090±0.035 | SI 0.238±0.020 | AdvG {1: 5}]\n",
      "Data: compas_sex | Model: DT_UnderBag  : 100%|██████████| 5/5 [00:01<00:00,  3.65it/s, ACC 0.629±0.015 | BACC 0.625±0.015 | DP 0.156±0.021 | EO 0.140±0.014 | SI 0.245±0.026 | AdvG {1: 5}]\n",
      "Data: compas_sex | Model: MLP          : 100%|██████████| 5/5 [00:01<00:00,  3.66it/s, ACC 0.686±0.014 | BACC 0.680±0.014 | DP 0.361±0.030 | EO 0.431±0.067 | SI 0.243±0.029 | AdvG {1: 5}]\n",
      "Data: compas_sex | Model: MLP_UnderBag : 100%|██████████| 5/5 [00:07<00:00,  1.59s/it, ACC 0.680±0.011 | BACC 0.679±0.010 | DP 0.408±0.017 | EO 0.437±0.036 | SI 0.281±0.010 | AdvG {1: 5}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Run Time on Dataset compas_sex: 0m14s ==========\n",
      "Results on Data: compas_sex\n",
      "Model: Dummy        | ACC 0.497±0.021            | BACC 0.495±0.022            | DP 0.010±0.006            | EO 0.044±0.043            | SI 0.000±0.000           \n",
      "Model: LR           | ACC 0.696±0.006            | BACC 0.690±0.006            | DP 0.336±0.016            | EO 0.382±0.039            | SI 0.210±0.013           \n",
      "Model: LR_UnderBag  | ACC 0.688±0.011 (-3.93%)   | BACC 0.688±0.011 (-1.02%)   | DP 0.377±0.034 (+12.25%)  | EO 0.371±0.057 (-3.03%)   | SI 0.248±0.013 (+17.99%)  | FURG -11.55   | FUTR -3.66   \n",
      "Model: DT           | ACC 0.607±0.013            | BACC 0.603±0.013            | DP 0.085±0.037            | EO 0.090±0.035            | SI 0.238±0.020           \n",
      "Model: DT_UnderBag  | ACC 0.629±0.015 (+19.60%)  | BACC 0.625±0.015 (+20.13%)  | DP 0.156±0.021 (+84.60%)  | EO 0.140±0.014 (+54.57%)  | SI 0.245±0.026 (+2.86%)   | FURG -27.48   | FUTR -47.34  \n",
      "Model: MLP          | ACC 0.686±0.014            | BACC 0.680±0.014            | DP 0.361±0.030            | EO 0.431±0.067            | SI 0.243±0.029           \n",
      "Model: MLP_UnderBag | ACC 0.680±0.011 (-3.14%)   | BACC 0.679±0.010 (-0.35%)   | DP 0.408±0.017 (+12.92%)  | EO 0.437±0.036 (+1.37%)   | SI 0.281±0.010 (+15.41%)  | FURG -11.64   | FUTR -5.67   \n",
      "========== Start Running on Dataset: compas_race ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data: compas_race | Model: Dummy        : 100%|██████████| 5/5 [00:00<00:00,  9.04it/s, ACC 0.491±0.015 | BACC 0.489±0.016 | DP 0.014±0.012 | EO 0.035±0.023 | SI 0.000±0.000 | AdvG {1: 3, 0: 2}]\n",
      "Data: compas_race | Model: LR           : 100%|██████████| 5/5 [00:00<00:00,  8.40it/s, ACC 0.685±0.011 | BACC 0.680±0.011 | DP 0.310±0.028 | EO 0.337±0.036 | SI 0.058±0.013 | AdvG {1: 5}]\n",
      "Data: compas_race | Model: LR_UnderBag  : 100%|██████████| 5/5 [00:01<00:00,  4.36it/s, ACC 0.673±0.010 | BACC 0.674±0.010 | DP 0.323±0.030 | EO 0.317±0.045 | SI 0.055±0.016 | AdvG {1: 5}]\n",
      "Data: compas_race | Model: DT           : 100%|██████████| 5/5 [00:00<00:00,  8.02it/s, ACC 0.614±0.009 | BACC 0.610±0.009 | DP 0.116±0.029 | EO 0.131±0.047 | SI 0.224±0.023 | AdvG {1: 5}]\n",
      "Data: compas_race | Model: DT_UnderBag  : 100%|██████████| 5/5 [00:01<00:00,  3.75it/s, ACC 0.626±0.012 | BACC 0.623±0.013 | DP 0.166±0.027 | EO 0.185±0.043 | SI 0.238±0.015 | AdvG {1: 5}]\n",
      "Data: compas_race | Model: MLP          : 100%|██████████| 5/5 [00:01<00:00,  3.74it/s, ACC 0.674±0.014 | BACC 0.667±0.014 | DP 0.335±0.038 | EO 0.371±0.047 | SI 0.102±0.027 | AdvG {1: 5}]\n",
      "Data: compas_race | Model: MLP_UnderBag : 100%|██████████| 5/5 [00:07<00:00,  1.55s/it, ACC 0.667±0.010 | BACC 0.667±0.010 | DP 0.359±0.029 | EO 0.362±0.044 | SI 0.095±0.016 | AdvG {1: 5}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Run Time on Dataset compas_race: 0m13s ==========\n",
      "Results on Data: compas_race\n",
      "Model: Dummy        | ACC 0.491±0.015            | BACC 0.489±0.016            | DP 0.014±0.012            | EO 0.035±0.023            | SI 0.000±0.000           \n",
      "Model: LR           | ACC 0.685±0.011            | BACC 0.680±0.011            | DP 0.310±0.028            | EO 0.337±0.036            | SI 0.058±0.013           \n",
      "Model: LR_UnderBag  | ACC 0.673±0.010 (-6.41%)   | BACC 0.674±0.010 (-3.52%)   | DP 0.323±0.030 (+4.40%)   | EO 0.317±0.045 (-6.13%)   | SI 0.055±0.016 (-5.25%)   | FURG -2.64    | FUTR 0.47    \n",
      "Model: DT           | ACC 0.614±0.009            | BACC 0.610±0.009            | DP 0.116±0.029            | EO 0.131±0.047            | SI 0.224±0.023           \n",
      "Model: DT_UnderBag  | ACC 0.626±0.012 (+9.85%)   | BACC 0.623±0.013 (+10.12%)  | DP 0.166±0.027 (+43.37%)  | EO 0.185±0.043 (+41.32%)  | SI 0.238±0.015 (+6.07%)   | FURG -20.28   | FUTR -30.26  \n",
      "Model: MLP          | ACC 0.674±0.014            | BACC 0.667±0.014            | DP 0.335±0.038            | EO 0.371±0.047            | SI 0.102±0.027           \n",
      "Model: MLP_UnderBag | ACC 0.667±0.010 (-3.45%)   | BACC 0.667±0.010 (-0.24%)   | DP 0.359±0.029 (+7.19%)   | EO 0.362±0.044 (-2.43%)   | SI 0.095±0.016 (-6.99%)   | FURG -1.10    | FUTR 0.40    \n",
      "========== Start Running on Dataset: adult_gender ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data: adult_gender | Model: Dummy        : 100%|██████████| 5/5 [00:01<00:00,  2.89it/s, ACC 0.624±0.004 | BACC 0.496±0.004 | DP 0.007±0.004 | EO 0.024±0.014 | SI 0.000±0.000 | AdvG {0.0: 3, 1.0: 2}]\n",
      "Data: adult_gender | Model: LR           : 100%|██████████| 5/5 [00:03<00:00,  1.39it/s, ACC 0.847±0.002 | BACC 0.765±0.003 | DP 0.189±0.011 | EO 0.130±0.038 | SI 0.083±0.007 | AdvG {1.0: 5}]\n",
      "Data: adult_gender | Model: LR_UnderBag  : 100%|██████████| 5/5 [00:13<00:00,  2.71s/it, ACC 0.801±0.004 | BACC 0.815±0.003 | DP 0.337±0.012 | EO 0.232±0.011 | SI 0.112±0.009 | AdvG {1.0: 5}]\n",
      "Data: adult_gender | Model: DT           : 100%|██████████| 5/5 [00:03<00:00,  1.60it/s, ACC 0.818±0.004 | BACC 0.749±0.006 | DP 0.179±0.010 | EO 0.088±0.006 | SI 0.051±0.007 | AdvG {1.0: 5}]\n",
      "Data: adult_gender | Model: DT_UnderBag  : 100%|██████████| 5/5 [00:13<00:00,  2.74s/it, ACC 0.823±0.003 | BACC 0.801±0.007 | DP 0.267±0.012 | EO 0.160±0.009 | SI 0.042±0.006 | AdvG {1.0: 5}]\n",
      "Data: adult_gender | Model: MLP          : 100%|██████████| 5/5 [00:15<00:00,  3.12s/it, ACC 0.847±0.002 | BACC 0.771±0.005 | DP 0.192±0.013 | EO 0.111±0.036 | SI 0.077±0.021 | AdvG {1.0: 5}]\n",
      "Data: adult_gender | Model: MLP_UnderBag : 100%|██████████| 5/5 [00:46<00:00,  9.35s/it, ACC 0.804±0.006 | BACC 0.819±0.003 | DP 0.329±0.012 | EO 0.225±0.011 | SI 0.099±0.014 | AdvG {1.0: 5}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Run Time on Dataset adult_gender: 1m38s ==========\n",
      "Results on Data: adult_gender\n",
      "Model: Dummy        | ACC 0.624±0.004            | BACC 0.496±0.004            | DP 0.007±0.004            | EO 0.024±0.014            | SI 0.000±0.000           \n",
      "Model: LR           | ACC 0.847±0.002            | BACC 0.765±0.003            | DP 0.189±0.011            | EO 0.130±0.038            | SI 0.083±0.007           \n",
      "Model: LR_UnderBag  | ACC 0.801±0.004 (-20.64%)  | BACC 0.815±0.003 (+18.58%)  | DP 0.337±0.012 (+77.90%)  | EO 0.232±0.011 (+78.26%)  | SI 0.112±0.009 (+35.83%)  | FURG -65.02   | FUTR -62.41  \n",
      "Model: DT           | ACC 0.818±0.004            | BACC 0.749±0.006            | DP 0.179±0.010            | EO 0.088±0.006            | SI 0.051±0.007           \n",
      "Model: DT_UnderBag  | ACC 0.823±0.003 (+2.79%)   | BACC 0.801±0.007 (+20.46%)  | DP 0.267±0.012 (+48.98%)  | EO 0.160±0.009 (+80.70%)  | SI 0.042±0.006 (-17.28%)  | FURG -25.84   | FUTR -37.47  \n",
      "Model: MLP          | ACC 0.847±0.002            | BACC 0.771±0.005            | DP 0.192±0.013            | EO 0.111±0.036            | SI 0.077±0.021           \n",
      "Model: MLP_UnderBag | ACC 0.804±0.006 (-19.20%)  | BACC 0.819±0.003 (+17.25%)  | DP 0.329±0.012 (+71.72%)  | EO 0.225±0.011 (+102.39%) | SI 0.099±0.014 (+28.36%)  | FURG -68.47   | FUTR -67.49  \n",
      "========== Start Running on Dataset: adult_race ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data: adult_race | Model: Dummy        : 100%|██████████| 5/5 [00:02<00:00,  2.50it/s, ACC 0.627±0.006 | BACC 0.501±0.005 | DP 0.012±0.004 | EO 0.031±0.015 | SI 0.000±0.000 | AdvG {1.0: 3, 0.0: 2}]\n",
      "Data: adult_race | Model: LR           : 100%|██████████| 5/5 [00:04<00:00,  1.21it/s, ACC 0.848±0.002 | BACC 0.764±0.004 | DP 0.102±0.004 | EO 0.086±0.031 | SI 0.018±0.003 | AdvG {1.0: 5}]\n",
      "Data: adult_race | Model: LR_UnderBag  : 100%|██████████| 5/5 [00:14<00:00,  2.86s/it, ACC 0.803±0.003 | BACC 0.816±0.003 | DP 0.179±0.011 | EO 0.123±0.015 | SI 0.024±0.004 | AdvG {1.0: 5}]\n",
      "Data: adult_race | Model: DT           : 100%|██████████| 5/5 [00:03<00:00,  1.60it/s, ACC 0.815±0.002 | BACC 0.748±0.005 | DP 0.084±0.012 | EO 0.039±0.012 | SI 0.068±0.006 | AdvG {1.0: 5}]\n",
      "Data: adult_race | Model: DT_UnderBag  : 100%|██████████| 5/5 [00:13<00:00,  2.75s/it, ACC 0.823±0.004 | BACC 0.801±0.004 | DP 0.147±0.007 | EO 0.092±0.009 | SI 0.065±0.004 | AdvG {1.0: 5}]\n",
      "Data: adult_race | Model: MLP          : 100%|██████████| 5/5 [00:15<00:00,  3.09s/it, ACC 0.848±0.002 | BACC 0.760±0.006 | DP 0.099±0.004 | EO 0.080±0.031 | SI 0.019±0.004 | AdvG {1.0: 5}]\n",
      "Data: adult_race | Model: MLP_UnderBag : 100%|██████████| 5/5 [00:54<00:00, 10.92s/it, ACC 0.806±0.002 | BACC 0.821±0.003 | DP 0.176±0.013 | EO 0.121±0.017 | SI 0.023±0.004 | AdvG {1.0: 5}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Run Time on Dataset adult_race: 1m47s ==========\n",
      "Results on Data: adult_race\n",
      "Model: Dummy        | ACC 0.627±0.006            | BACC 0.501±0.005            | DP 0.012±0.004            | EO 0.031±0.015            | SI 0.000±0.000           \n",
      "Model: LR           | ACC 0.848±0.002            | BACC 0.764±0.004            | DP 0.102±0.004            | EO 0.086±0.031            | SI 0.018±0.003           \n",
      "Model: LR_UnderBag  | ACC 0.803±0.003 (-20.17%)  | BACC 0.816±0.003 (+19.83%)  | DP 0.179±0.011 (+75.78%)  | EO 0.123±0.015 (+43.44%)  | SI 0.024±0.004 (+33.25%)  | FURG -50.99   | FUTR -50.82  \n",
      "Model: DT           | ACC 0.815±0.002            | BACC 0.748±0.005            | DP 0.084±0.012            | EO 0.039±0.012            | SI 0.068±0.006           \n",
      "Model: DT_UnderBag  | ACC 0.823±0.004 (+4.35%)   | BACC 0.801±0.004 (+21.51%)  | DP 0.147±0.007 (+74.73%)  | EO 0.092±0.009 (+134.77%) | SI 0.065±0.004 (-4.21%)   | FURG -55.50   | FUTR -68.43  \n",
      "Model: MLP          | ACC 0.848±0.002            | BACC 0.760±0.006            | DP 0.099±0.004            | EO 0.080±0.031            | SI 0.019±0.004           \n",
      "Model: MLP_UnderBag | ACC 0.806±0.002 (-19.21%)  | BACC 0.821±0.003 (+23.39%)  | DP 0.176±0.013 (+78.70%)  | EO 0.121±0.017 (+50.76%)  | SI 0.023±0.004 (+18.52%)  | FURG -47.24   | FUTR -49.33  \n",
      "========== Start Running on Dataset: bank_age ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data: bank_age | Model: Dummy        : 100%|██████████| 5/5 [00:01<00:00,  2.63it/s, ACC 0.777±0.003 | BACC 0.499±0.006 | DP 0.016±0.016 | EO 0.052±0.037 | SI 0.000±0.000 | AdvG {0: 5}]\n",
      "Data: bank_age | Model: LR           : 100%|██████████| 5/5 [00:03<00:00,  1.48it/s, ACC 0.897±0.003 | BACC 0.678±0.005 | DP 0.103±0.033 | EO 0.144±0.106 | SI 0.011±0.003 | AdvG {1: 5}]\n",
      "Data: bank_age | Model: LR_UnderBag  : 100%|██████████| 5/5 [00:08<00:00,  1.69s/it, ACC 0.850±0.005 | BACC 0.849±0.007 | DP 0.245±0.028 | EO 0.191±0.036 | SI 0.023±0.007 | AdvG {1: 5}]\n",
      "Data: bank_age | Model: DT           : 100%|██████████| 5/5 [00:03<00:00,  1.55it/s, ACC 0.878±0.004 | BACC 0.725±0.005 | DP 0.099±0.028 | EO 0.115±0.067 | SI 0.024±0.003 | AdvG {1: 5}]\n",
      "Data: bank_age | Model: DT_UnderBag  : 100%|██████████| 5/5 [00:08<00:00,  1.68s/it, ACC 0.873±0.003 | BACC 0.857±0.007 | DP 0.148±0.041 | EO 0.091±0.056 | SI 0.017±0.004 | AdvG {1: 5}]\n",
      "Data: bank_age | Model: MLP          : 100%|██████████| 5/5 [00:18<00:00,  3.66s/it, ACC 0.900±0.003 | BACC 0.708±0.014 | DP 0.113±0.030 | EO 0.140±0.077 | SI 0.014±0.006 | AdvG {1: 5}]\n",
      "Data: bank_age | Model: MLP_UnderBag : 100%|██████████| 5/5 [00:14<00:00,  2.97s/it, ACC 0.845±0.005 | BACC 0.832±0.006 | DP 0.249±0.024 | EO 0.196±0.026 | SI 0.017±0.008 | AdvG {1: 5}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Run Time on Dataset bank_age: 0m59s ==========\n",
      "Results on Data: bank_age\n",
      "Model: Dummy        | ACC 0.777±0.003            | BACC 0.499±0.006            | DP 0.016±0.016            | EO 0.052±0.037            | SI 0.000±0.000           \n",
      "Model: LR           | ACC 0.897±0.003            | BACC 0.678±0.005            | DP 0.103±0.033            | EO 0.144±0.106            | SI 0.011±0.003           \n",
      "Model: LR_UnderBag  | ACC 0.850±0.005 (-38.77%)  | BACC 0.849±0.007 (+95.54%)  | DP 0.245±0.028 (+137.95%) | EO 0.191±0.036 (+32.52%)  | SI 0.023±0.007 (+109.73%) | FURG -65.02   | FUTR -93.40  \n",
      "Model: DT           | ACC 0.878±0.004            | BACC 0.725±0.005            | DP 0.099±0.028            | EO 0.115±0.067            | SI 0.024±0.003           \n",
      "Model: DT_UnderBag  | ACC 0.873±0.003 (-5.14%)   | BACC 0.857±0.007 (+58.65%)  | DP 0.148±0.041 (+49.24%)  | EO 0.091±0.056 (-20.69%)  | SI 0.017±0.004 (-26.82%)  | FURG 26.18    | FUTR -0.58   \n",
      "Model: MLP          | ACC 0.900±0.003            | BACC 0.708±0.014            | DP 0.113±0.030            | EO 0.140±0.077            | SI 0.014±0.006           \n",
      "Model: MLP_UnderBag | ACC 0.845±0.005 (-44.52%)  | BACC 0.832±0.006 (+58.92%)  | DP 0.249±0.024 (+120.80%) | EO 0.196±0.026 (+39.93%)  | SI 0.017±0.008 (+23.39%)  | FURG -54.17   | FUTR -61.37  \n",
      "========== Start Running on Dataset: bank_marital=married ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data: bank_marital=married | Model: Dummy        : 100%|██████████| 5/5 [00:01<00:00,  4.72it/s, ACC 0.778±0.003 | BACC 0.501±0.002 | DP 0.004±0.003 | EO 0.021±0.021 | SI 0.000±0.000 | AdvG {0: 3, 1: 2}]\n",
      "Data: bank_marital=married | Model: LR           : 100%|██████████| 5/5 [00:01<00:00,  2.80it/s, ACC 0.900±0.001 | BACC 0.688±0.006 | DP 0.021±0.006 | EO 0.037±0.031 | SI 0.001±0.001 | AdvG {1: 5}]\n",
      "Data: bank_marital=married | Model: LR_UnderBag  : 100%|██████████| 5/5 [00:04<00:00,  1.09it/s, ACC 0.852±0.003 | BACC 0.855±0.004 | DP 0.056±0.007 | EO 0.051±0.016 | SI 0.003±0.002 | AdvG {1: 5}]\n",
      "Data: bank_marital=married | Model: DT           : 100%|██████████| 5/5 [00:01<00:00,  2.73it/s, ACC 0.877±0.002 | BACC 0.731±0.002 | DP 0.027±0.010 | EO 0.033±0.023 | SI 0.018±0.006 | AdvG {1: 5}]\n",
      "Data: bank_marital=married | Model: DT_UnderBag  : 100%|██████████| 5/5 [00:05<00:00,  1.04s/it, ACC 0.873±0.002 | BACC 0.865±0.009 | DP 0.036±0.011 | EO 0.023±0.011 | SI 0.012±0.002 | AdvG {1: 5}]\n",
      "Data: bank_marital=married | Model: MLP          : 100%|██████████| 5/5 [00:07<00:00,  1.46s/it, ACC 0.903±0.002 | BACC 0.721±0.023 | DP 0.026±0.007 | EO 0.044±0.029 | SI 0.014±0.007 | AdvG {1: 5}]\n",
      "Data: bank_marital=married | Model: MLP_UnderBag : 100%|██████████| 5/5 [00:14<00:00,  2.97s/it, ACC 0.847±0.004 | BACC 0.837±0.002 | DP 0.052±0.007 | EO 0.052±0.023 | SI 0.014±0.006 | AdvG {1: 5}]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Run Time on Dataset bank_marital=married: 0m37s ==========\n",
      "Results on Data: bank_marital=married\n",
      "Model: Dummy        | ACC 0.778±0.003            | BACC 0.501±0.002            | DP 0.004±0.003            | EO 0.021±0.021            | SI 0.000±0.000           \n",
      "Model: LR           | ACC 0.900±0.001            | BACC 0.688±0.006            | DP 0.021±0.006            | EO 0.037±0.031            | SI 0.001±0.001           \n",
      "Model: LR_UnderBag  | ACC 0.852±0.003 (-38.88%)  | BACC 0.855±0.004 (+89.21%)  | DP 0.056±0.007 (+163.81%) | EO 0.051±0.016 (+39.71%)  | SI 0.003±0.002 (+254.17%) | FURG -127.40  | FUTR -152.56 \n",
      "Model: DT           | ACC 0.877±0.002            | BACC 0.731±0.002            | DP 0.027±0.010            | EO 0.033±0.023            | SI 0.018±0.006           \n",
      "Model: DT_UnderBag  | ACC 0.873±0.002 (-4.02%)   | BACC 0.865±0.009 (+58.23%)  | DP 0.036±0.011 (+32.85%)  | EO 0.023±0.011 (-29.45%)  | SI 0.012±0.002 (-31.84%)  | FURG 36.59    | FUTR 9.48    \n",
      "Model: MLP          | ACC 0.903±0.002            | BACC 0.721±0.023            | DP 0.026±0.007            | EO 0.044±0.029            | SI 0.014±0.007           \n",
      "Model: MLP_UnderBag | ACC 0.847±0.004 (-44.57%)  | BACC 0.837±0.002 (+52.27%)  | DP 0.052±0.007 (+101.89%) | EO 0.052±0.023 (+17.75%)  | SI 0.014±0.006 (-2.31%)   | FURG -35.25   | FUTR -39.11  \n",
      "========== Total Run Time: 5m28s ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from trainer import Benchmarker\n",
    "from baselines import ReweightClassifier, ReductionClassifier\n",
    "from imbens.ensemble import UnderBaggingClassifier\n",
    "\n",
    "ensemble_kwargs = {\n",
    "    'n_estimators': 10,\n",
    "    'random_state': 42,\n",
    "}\n",
    "single_ensemble_kwargs = {\n",
    "    'n_estimators': 1,\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "base_models = {\n",
    "    'LR': LogisticRegression(),\n",
    "    # 'KN': KNeighborsClassifier(),\n",
    "    'DT': DecisionTreeClassifier(max_depth=None),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(8), max_iter=50),\n",
    "    # 'ADA': AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=None), n_estimators=5),\n",
    "    # 'BAG': BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=None), n_estimators=5),\n",
    "}\n",
    "\n",
    "baselines = {\n",
    "    # 'AdaBoost': (AdaBoostClassifier, {**ensemble_kwargs}),\n",
    "    # 'Bagging': (BaggingClassifier, {**ensemble_kwargs}),\n",
    "    # 'RUSBoost': (RUSBoostClassifier, {**ensemble_kwargs}),\n",
    "    # 'UnderBag': (UnderBaggingClassifier, {**ensemble_kwargs}),\n",
    "    # 'SMBoost': (SMOTEBoostClassifier, {**ensemble_kwargs}),\n",
    "    # 'SMBag': (SMOTEBaggingClassifier, {**ensemble_kwargs}),\n",
    "    # 'Reweight': (ReweightClassifier, {}),\n",
    "    # 'ReductionDP': (ReductionClassifier, {'constraints': 'DemographicParity'}),\n",
    "    # 'ReductionEO': (ReductionClassifier, {'constraints': 'EqualizedOdds'}),\n",
    "    # 'ThresDP': (ThresholdOptimizer, {'constraints': 'demographic_parity'}),\n",
    "    # 'ThresEO': (ThresholdOptimizer, {'constraints': 'equalized_odds'}),\n",
    "    # 'AdaFair': (AdaFairClassifier, {'saIndex': 0, 'saValue': 0, 'CSB': 'CSB2', **ensemble_kwargs}),\n",
    "    # 'DisparateIR': (DisparateImpactRemovalClassifier, {'repair_level': 1.0}),\n",
    "    # 'LFR': (LFRClassifer, {'privileged_groups': [{'0': 1}], 'unprivileged_groups': [{'0': 0}]}),\n",
    "    'UnderBag': (UnderBaggingClassifier, {**ensemble_kwargs}),\n",
    "}\n",
    "\n",
    "benchmark = Benchmarker(\n",
    "    base_models=base_models,\n",
    "    baselines=baselines,\n",
    "    datasets=dataset_zoo_subset,\n",
    "    random_state=42,\n",
    "    dummy_strategy='stratified',\n",
    ")\n",
    "benchmark.run(\n",
    "    n_runs=5, \n",
    "    group_by='dataset', \n",
    "    exception='raise',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\n",
      "`load_boston` has been removed from scikit-learn since version 1.2.\n",
      "\n",
      "The Boston housing prices dataset has an ethical problem: as\n",
      "investigated in [1], the authors of this dataset engineered a\n",
      "non-invertible variable \"B\" assuming that racial self-segregation had a\n",
      "positive impact on house prices [2]. Furthermore the goal of the\n",
      "research that led to the creation of this dataset was to study the\n",
      "impact of air quality but it did not give adequate demonstration of the\n",
      "validity of this assumption.\n",
      "\n",
      "The scikit-learn maintainers therefore strongly discourage the use of\n",
      "this dataset unless the purpose of the code is to study and educate\n",
      "about ethical issues in data science and machine learning.\n",
      "\n",
      "In this special case, you can fetch the dataset from the original\n",
      "source::\n",
      "\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "    target = raw_df.values[1::2, 2]\n",
      "\n",
      "Alternative datasets include the California housing dataset and the\n",
      "Ames housing dataset. You can load the datasets as follows::\n",
      "\n",
      "    from sklearn.datasets import fetch_california_housing\n",
      "    housing = fetch_california_housing()\n",
      "\n",
      "for the California housing dataset and::\n",
      "\n",
      "    from sklearn.datasets import fetch_openml\n",
      "    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "for the Ames housing dataset.\n",
      "\n",
      "[1] M Carlisle.\n",
      "\"Racist data destruction?\"\n",
      "<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n",
      "\n",
      "[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n",
      "\"Hedonic housing prices and the demand for clean air.\"\n",
      "Journal of environmental economics and management 5.1 (1978): 81-102.\n",
      "<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
      ": fetch_lawschool_gpa will be unavailable. To install, run:\n",
      "pip install 'aif360[LawSchoolGPA]'\n"
     ]
    }
   ],
   "source": [
    "from aif360.sklearn.datasets import fetch_adult\n",
    "from aif360.datasets import AdultDataset, BankDataset, BinaryLabelDataset\n",
    "\n",
    "# fetch_adult()\n",
    "# ad = AdultDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC as SVM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "from aif360.datasets import AdultDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import base classifiers\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from baselines import AdaFairClassifier\n",
    "from imbens.ensemble import SMOTEBoostClassifier, SMOTEBaggingClassifier, RUSBoostClassifier, UnderBaggingClassifier, SelfPacedEnsembleClassifier\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairens import FairAugEnsemble, FairEnsemble\n",
    "\n",
    "# import aif360 \n",
    "import aif360\n",
    "\n",
    "# Import utilities\n",
    "from data import FairDataset    # This is a custom class that we will use to load the datasets\n",
    "from eval import evaluate_multi_split, verbose_print\n",
    "from trainer import Trainer\n",
    "from utils import seed_generator, dict_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset    : adult (45222, 99) load from ./data/adult.csv\n",
      "Sens/Res   : gender/label\n",
      "Split      : train/test = 0.8/0.2, random_state = 42, x_with_s = True\n",
      "train data [#samples 36177 #features 98]:\n",
      "+-----+-------+-------+------------+\n",
      "|     |   y=0 |   y=1 |   pos_rate |\n",
      "+=====+=======+=======+============+\n",
      "| s=0 | 10421 |  1335 |     0.1136 |\n",
      "+-----+-------+-------+------------+\n",
      "| s=1 | 16790 |  7631 |     0.3125 |\n",
      "+-----+-------+-------+------------+\n",
      "test data [#samples 9045 #features 98]:\n",
      "+-----+-------+-------+------------+\n",
      "|     |   y=0 |   y=1 |   pos_rate |\n",
      "+=====+=======+=======+============+\n",
      "| s=0 |  2605 |   334 |     0.1136 |\n",
      "+-----+-------+-------+------------+\n",
      "| s=1 |  4198 |  1908 |     0.3125 |\n",
      "+-----+-------+-------+------------+\n",
      "\n",
      "(27132, 98) (9045, 98) (9045, 98)\n",
      "{'acc': 0.8670573492554917, 'bacc': 0.7777528775851861, 'dp': 0.15729116477670407, 'eo': 0.04937490164507953, 'si': 0.0911469851098334, 'acc_grp': {0.0: 0.937, 1.0: 0.833}, 'pos_rate_grp': {0.0: 0.077, 1.0: 0.234}, 'g_adv': 1.0, 'acc_cls': {0.0: 0.955, 1.0: 0.601}}\n",
      "{'acc': 0.844776119402985, 'bacc': 0.7495348250489794, 'dp': 0.16359858670129293, 'eo': 0.06932722738823624, 'si': 0.09342177998894417, 'acc_grp': {0.0: 0.928, 1.0: 0.805}, 'pos_rate_grp': {0.0: 0.075, 1.0: 0.238}, 'g_adv': 1.0, 'acc_cls': {0.0: 0.938, 1.0: 0.561}}\n"
     ]
    }
   ],
   "source": [
    "from eval import flip_s_in_X, evaluate\n",
    "\n",
    "dataset_kwargs = {\n",
    "    'y_col': 'label',\n",
    "    'train_size': 0.6,\n",
    "    'val_size': 0.2,\n",
    "    'test_size': 0.2,\n",
    "    'concat_train_val': True,\n",
    "    'normalize': True,\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "dataname = 'adult'\n",
    "s_attr = 'gender'\n",
    "data = FairDataset(\n",
    "    dataname=dataname,\n",
    "    csv_path=f'./data/{dataname}.csv',\n",
    "    s_col=s_attr,\n",
    "    x_with_s=True,\n",
    "    **dataset_kwargs\n",
    ")\n",
    "\n",
    "data.describe()\n",
    "\n",
    "(\n",
    "    (X_train, y_train, s_train),\n",
    "    (X_val, y_val, s_val),\n",
    "    (X_test, y_test, s_test),\n",
    ") = data.train_val_test_split(x_with_s=True)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "n_feat = X_train.shape[1]\n",
    "n_class = len(classes)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(32), max_iter=500)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(evaluate(clf, X_train, y_train, s_train))\n",
    "print(evaluate(clf, X_test, y_test, s_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass=Federal-gov</th>\n",
       "      <th>workclass=Local-gov</th>\n",
       "      <th>workclass=Private</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country=Puerto-Rico</th>\n",
       "      <th>native-country=Scotland</th>\n",
       "      <th>native-country=South</th>\n",
       "      <th>native-country=Taiwan</th>\n",
       "      <th>native-country=Thailand</th>\n",
       "      <th>native-country=Trinadad&amp;Tobago</th>\n",
       "      <th>native-country=United-States</th>\n",
       "      <th>native-country=Vietnam</th>\n",
       "      <th>native-country=Yugoslavia</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.021740</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.301370</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.452055</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.287671</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45217</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45218</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.301370</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45219</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.287671</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45220</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.054551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369863</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45221</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.246575</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.602041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45222 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gender  capital-gain  race       age  education-num  capital-loss  \\\n",
       "0         1.0      0.021740   1.0  0.301370       0.800000           0.0   \n",
       "1         1.0      0.000000   1.0  0.452055       0.800000           0.0   \n",
       "2         1.0      0.000000   1.0  0.287671       0.533333           0.0   \n",
       "3         1.0      0.000000   0.0  0.493151       0.400000           0.0   \n",
       "4         0.0      0.000000   0.0  0.150685       0.800000           0.0   \n",
       "...       ...           ...   ...       ...            ...           ...   \n",
       "45217     1.0      0.000000   1.0  0.219178       0.800000           0.0   \n",
       "45218     0.0      0.000000   1.0  0.301370       0.800000           0.0   \n",
       "45219     1.0      0.000000   1.0  0.287671       0.800000           0.0   \n",
       "45220     1.0      0.054551   0.0  0.369863       0.800000           0.0   \n",
       "45221     1.0      0.000000   1.0  0.246575       0.800000           0.0   \n",
       "\n",
       "       hours-per-week  workclass=Federal-gov  workclass=Local-gov  \\\n",
       "0            0.397959                    0.0                  0.0   \n",
       "1            0.122449                    0.0                  0.0   \n",
       "2            0.397959                    0.0                  0.0   \n",
       "3            0.397959                    0.0                  0.0   \n",
       "4            0.397959                    0.0                  0.0   \n",
       "...               ...                    ...                  ...   \n",
       "45217        0.397959                    0.0                  0.0   \n",
       "45218        0.357143                    0.0                  0.0   \n",
       "45219        0.500000                    0.0                  0.0   \n",
       "45220        0.397959                    0.0                  0.0   \n",
       "45221        0.602041                    0.0                  0.0   \n",
       "\n",
       "       workclass=Private  ...  native-country=Puerto-Rico  \\\n",
       "0                    0.0  ...                         0.0   \n",
       "1                    0.0  ...                         0.0   \n",
       "2                    1.0  ...                         0.0   \n",
       "3                    1.0  ...                         0.0   \n",
       "4                    1.0  ...                         0.0   \n",
       "...                  ...  ...                         ...   \n",
       "45217                1.0  ...                         0.0   \n",
       "45218                1.0  ...                         0.0   \n",
       "45219                1.0  ...                         0.0   \n",
       "45220                1.0  ...                         0.0   \n",
       "45221                0.0  ...                         0.0   \n",
       "\n",
       "       native-country=Scotland  native-country=South  native-country=Taiwan  \\\n",
       "0                          0.0                   0.0                    0.0   \n",
       "1                          0.0                   0.0                    0.0   \n",
       "2                          0.0                   0.0                    0.0   \n",
       "3                          0.0                   0.0                    0.0   \n",
       "4                          0.0                   0.0                    0.0   \n",
       "...                        ...                   ...                    ...   \n",
       "45217                      0.0                   0.0                    0.0   \n",
       "45218                      0.0                   0.0                    0.0   \n",
       "45219                      0.0                   0.0                    0.0   \n",
       "45220                      0.0                   0.0                    0.0   \n",
       "45221                      0.0                   0.0                    0.0   \n",
       "\n",
       "       native-country=Thailand  native-country=Trinadad&Tobago  \\\n",
       "0                          0.0                             0.0   \n",
       "1                          0.0                             0.0   \n",
       "2                          0.0                             0.0   \n",
       "3                          0.0                             0.0   \n",
       "4                          0.0                             0.0   \n",
       "...                        ...                             ...   \n",
       "45217                      0.0                             0.0   \n",
       "45218                      0.0                             0.0   \n",
       "45219                      0.0                             0.0   \n",
       "45220                      0.0                             0.0   \n",
       "45221                      0.0                             0.0   \n",
       "\n",
       "       native-country=United-States  native-country=Vietnam  \\\n",
       "0                               1.0                     0.0   \n",
       "1                               1.0                     0.0   \n",
       "2                               1.0                     0.0   \n",
       "3                               1.0                     0.0   \n",
       "4                               0.0                     0.0   \n",
       "...                             ...                     ...   \n",
       "45217                           1.0                     0.0   \n",
       "45218                           1.0                     0.0   \n",
       "45219                           1.0                     0.0   \n",
       "45220                           1.0                     0.0   \n",
       "45221                           1.0                     0.0   \n",
       "\n",
       "       native-country=Yugoslavia  label  \n",
       "0                            0.0    0.0  \n",
       "1                            0.0    0.0  \n",
       "2                            0.0    0.0  \n",
       "3                            0.0    0.0  \n",
       "4                            0.0    0.0  \n",
       "...                          ...    ...  \n",
       "45217                        0.0    0.0  \n",
       "45218                        0.0    0.0  \n",
       "45219                        0.0    0.0  \n",
       "45220                        0.0    0.0  \n",
       "45221                        0.0    1.0  \n",
       "\n",
       "[45222 rows x 99 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_repd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ZhiningLiu\\OneDrive\\_Github\\FairEdit\\aif360_pipeline.ipynb 单元格 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ZhiningLiu/OneDrive/_Github/FairEdit/aif360_pipeline.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m bias_remover \u001b[39m=\u001b[39m DisparateImpactRemover(repair_level\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ZhiningLiu/OneDrive/_Github/FairEdit/aif360_pipeline.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m debiased_data_train \u001b[39m=\u001b[39m bias_remover\u001b[39m.\u001b[39mfit_transform(data_train)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ZhiningLiu/OneDrive/_Github/FairEdit/aif360_pipeline.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m((train_repd\u001b[39m.\u001b[39mfeatures \u001b[39m!=\u001b[39m X_train)\u001b[39m.\u001b[39msum() \u001b[39m/\u001b[39m X_train\u001b[39m.\u001b[39msize)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_repd' is not defined"
     ]
    }
   ],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "df_train = pd.DataFrame(np.hstack([X_train, y_train.reshape(-1, 1)]))\n",
    "df_test = pd.DataFrame(np.hstack([X_test, y_test.reshape(-1, 1)]))\n",
    "\n",
    "data_train = BinaryLabelDataset(\n",
    "    df=pd.DataFrame(np.hstack([X_train, y_train.reshape(-1, 1)])), \n",
    "    label_names=[df_train.columns[-1]],\n",
    "    protected_attribute_names=[0],\n",
    "    privileged_protected_attributes = [[1.0]],\n",
    "    unprivileged_protected_attributes = [[0.0]],\n",
    ")\n",
    "bias_remover = DisparateImpactRemover(repair_level=1.0)\n",
    "debiased_data_train = bias_remover.fit_transform(data_train)\n",
    "\n",
    "print((train_repd.features != X_train).sum() / X_train.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate whether the 1st column of X is the sensitive attribute\n",
    "(train_repd.features[:, 0] == s_train).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "\n",
    "class DisparateImpactRemovalClassifier(BaseEstimator):\n",
    "\n",
    "    def __init__(self, estimator, repair_level:int=1.0, verbose=False, random_state=None):\n",
    "        assert isinstance(estimator, ClassifierMixin), \"estimator must be a classifier\"\n",
    "        assert repair_level >= 0.0 and repair_level <= 1.0, \"repair_level must be in [0, 1]\"\n",
    "        assert isinstance(verbose, bool), \"verbose must be a boolean\"\n",
    "        assert isinstance(random_state, int) or random_state is None, \"random_state must be an integer or None\"\n",
    "                \n",
    "        self.estimator = estimator\n",
    "        self.repair_level = repair_level\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self.set_params(random_state=random_state)\n",
    "\n",
    "    def set_params(self, **kwargs):\n",
    "        try:\n",
    "            self.estimator.set_params(**kwargs)\n",
    "        except:\n",
    "            pass\n",
    " \n",
    "    def fit(self, X, y, sensitive_features):\n",
    "        self.preprocessor = DisparateImpactRemover(\n",
    "            repair_level=self.repair_level, sensitive_attribute=0\n",
    "        )\n",
    "        X_processed = self.remove_bias(X, sensitive_features)\n",
    "        self.estimator.fit(X_processed, y)\n",
    "        return self\n",
    "\n",
    "    def remove_bias(self, X, sensitive_features):\n",
    "        \"\"\"Remove bias from X using the DisparateImpactRemover preprocessor.\"\"\"\n",
    "        assert (X[:, 0] == sensitive_features).all(), \\\n",
    "            \"The 1st column of X must be the sensitive attribute.\"\n",
    "        \n",
    "        y_dummy = np.zeros(X.shape[0])\n",
    "        aif360_data = BinaryLabelDataset(\n",
    "            df=pd.DataFrame(np.hstack([X, y_dummy.reshape(-1, 1)])), \n",
    "            label_names=[X.shape[1]],\n",
    "            protected_attribute_names=[0],\n",
    "            privileged_protected_attributes = [[1.0]],\n",
    "            unprivileged_protected_attributes = [[0.0]],\n",
    "        )\n",
    "        X_processed = self.preprocessor.fit_transform(aif360_data).features\n",
    "        return X_processed\n",
    "\n",
    "    def predict(self, X, sensitive_features):\n",
    "        X_processed = self.remove_bias(X, sensitive_features)\n",
    "        return self.estimator.predict(X_processed)\n",
    "\n",
    "    def predict_proba(self, X, sensitive_features):\n",
    "        X_processed = self.remove_bias(X, sensitive_features)\n",
    "        return self.estimator.predict_proba(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_clf = LogisticRegression()\n",
    "clf = DisparateImpactRemovalClassifier(base_clf, repair_level=1.0)\n",
    "\n",
    "clf.fit(X_train, y_train, s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# transform into aif360 dataset\n",
    "df_train = pd.DataFrame(X_train, columns=data.df.columns[:-1])\n",
    "df_train['label'] = y_train\n",
    "df_test = pd.DataFrame(X_test, columns=data.df.columns[:-1])\n",
    "df_test['label'] = y_test\n",
    "\n",
    "\n",
    "data_train = BinaryLabelDataset(\n",
    "    df=df_train, label_names=['label'], protected_attribute_names=['gender'],\n",
    "    privileged_protected_attributes = [[1.0]],\n",
    "    unprivileged_protected_attributes = [[0.0]],\n",
    "    )\n",
    "index = data_train.feature_names.index('gender')\n",
    "data_test = BinaryLabelDataset(df=df_test, label_names=['label'], protected_attribute_names=['gender'])\n",
    "\n",
    "di = DisparateImpactRemover(repair_level=1.0,sensitive_attribute=0)\n",
    "train_repd = di.fit_transform(data_train)\n",
    "test_repd = di.fit_transform(data_test)\n",
    "\n",
    "X_tr = np.delete(train_repd.features, index, axis=1)\n",
    "X_te = np.delete(test_repd.features, index, axis=1)\n",
    "y_tr = train_repd.labels.ravel()\n",
    "\n",
    "X_tr.shape, X_te.shape, y_tr.shape\n",
    "\n",
    "(train_repd.features != X_train).sum() / X_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_repd.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = MLPClassifier(hidden_layer_sizes=(32), max_iter=500)\n",
    "clf = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
    "\n",
    "clf.fit(X_tr, y_train)\n",
    "\n",
    "print(evaluate(clf, X_tr, y_train, s_train))\n",
    "print(evaluate(clf, X_te, y_test, s_test))\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(evaluate(clf, X_train, y_train, s_train))\n",
    "print(evaluate(clf, X_test, y_test, s_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, MetaEstimatorMixin, clone\n",
    "from sklearn.utils.validation import has_fit_parameter\n",
    "\n",
    "from aif360.sklearn.utils import check_inputs, check_groups\n",
    "\n",
    "class ReweightClassifier(BaseEstimator):\n",
    "    def __init__(self, estimator, random_state=None):\n",
    "        self.estimator = estimator\n",
    "        self.random_state = random_state\n",
    "        self.set_params(random_state=random_state)\n",
    "\n",
    "    def set_params(self, **kwargs):\n",
    "        try:\n",
    "            self.estimator.set_params(**kwargs)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        preprocessor = Reweighing(prot_attr=0)\n",
    "        _, sample_weight = preprocessor.fit_transform(pd.DataFrame(X_train), y_train)\n",
    "        self.estimator.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.estimator.predict(X_test)\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        return self.estimator.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
